

# Idea One

How does interpretable features extracted by sparse autoencoders change across different model scales and training steps 

# Idea Two

Is it possible to identify "Circuits" in language models that mirror biological circuits for mathematical cognition.

# Idea Three

Sparse autoencoders -> Still has polysemanticity
Is there any way to reduce the polysemanticity autoencoders

Copy surpression attention head
Classifier for unfaithful output with the help of attention graph
Quantification of features "Closeness" in dictionary learning/Sparse Autoencoders
Use feature extraction to dissect how LLMs solve complex mathematical problems