

# Idea One

How does interpretable features extracted by sparse autoencoders change across different model scales and training steps 

# Idea Two

Is it possible to identify "Circuits" in language models that mirror biological circuits for mathematical cognition.

# Idea Three

Sparse autoencoders -> Still has polysemanticity
Is there any way to reduce the polysemanticity autoencoders

