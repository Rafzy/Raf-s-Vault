

# Idea One

How does interpretable features extracted by sparse autoencoders change across different model scales and training steps 

# Idea Two

Is it possible to identify "Circuits" in language models that mirror biological circuits for mathematical cognition.

